{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04830f05-b59e-41a1-83eb-e07e263f5263",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('name', 'test_restoration_mse'), ('suffix', None), ('scale', 4), ('model_type', 'RefRestorationModel'), ('set_CUDA_VISIBLE_DEVICES', None), ('crop_border', 4), ('gpu_ids', None), ('datasets', OrderedDict([('test_1', OrderedDict([('name', 'WR-SR'), ('type', 'RefCUFEDDataset'), ('dataroot_in', 'datasets\\\\WR-SR\\\\input'), ('dataroot_ref', 'datasets\\\\WR-SR\\\\ref'), ('io_backend', OrderedDict([('type', 'disk')])), ('bicubic_model', 'PIL'), ('ann_file', 'datasets\\\\WR-SR_pairs.txt')]))])), ('val_func', 'BasicSRValidation'), ('save_img', False), ('network_g', OrderedDict([('type', 'SwinUnetv3RestorationNet'), ('ngf', 128), ('n_blocks', 8), ('groups', 8), ('embed_dim', 128), ('depths', [4, 4]), ('num_heads', [4, 4]), ('window_size', 8), ('use_checkpoint', True)])), ('network_map', OrderedDict([('type', 'FlowSimCorrespondenceGenerationArch'), ('patch_size', 3), ('stride', 1), ('vgg_layer_list', ['relu1_1', 'relu2_1', 'relu3_1']), ('vgg_type', 'vgg19')])), ('network_extractor', OrderedDict([('type', 'ContrasExtractorSep')])), ('path', OrderedDict([('pretrain_model_g', 'experiments/pretrained_model/restoration_mse.pth'), ('pretrain_model_feature_extractor', 'experiments/feature_extraction.pth'), ('strict_load', True), ('root', 'experiments/test/')])), ('is_train', False)])\n",
      "Disabled distributed testing.\n",
      "Path already exists. Rename it to experiments/test/results\\test_restoration_mse_archived_20241117_130533\n",
      "[                              ] 0/2, elapsed: 0s, ETA:\n",
      "Start...\n",
      "\u001b[2F\u001b[J[>>>>>>>>>>>>>>>---------------] 1/2, 0.0 task/s, elapsed: 205s, ETA:   205s\n",
      "Test 001_ref\n",
      "\u001b[2F\u001b[J[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 2/2, 0.0 task/s, elapsed: 640s, ETA:     0s\n",
      "Test 002_ref\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dheer\\.conda\\envs\\mmcv\\lib\\site-packages\\mmcv\\__init__.py:21: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  'On January 1, 2023, MMCV will release v2.0.0, in which it will remove '\n",
      "2024-11-17 13:05:33,090.090 - INFO:   name: test_restoration_mse\n",
      "  suffix: None\n",
      "  scale: 4\n",
      "  model_type: RefRestorationModel\n",
      "  set_CUDA_VISIBLE_DEVICES: None\n",
      "  crop_border: 4\n",
      "  gpu_ids: None\n",
      "  datasets:[\n",
      "    test_1:[\n",
      "      name: WR-SR\n",
      "      type: RefCUFEDDataset\n",
      "      dataroot_in: datasets\\WR-SR\\input\n",
      "      dataroot_ref: datasets\\WR-SR\\ref\n",
      "      io_backend:[\n",
      "        type: disk\n",
      "      ]\n",
      "      bicubic_model: PIL\n",
      "      ann_file: datasets\\WR-SR_pairs.txt\n",
      "      phase: test\n",
      "      scale: 4\n",
      "    ]\n",
      "  ]\n",
      "  val_func: BasicSRValidation\n",
      "  save_img: False\n",
      "  network_g:[\n",
      "    type: SwinUnetv3RestorationNet\n",
      "    ngf: 128\n",
      "    n_blocks: 8\n",
      "    groups: 8\n",
      "    embed_dim: 128\n",
      "    depths: [4, 4]\n",
      "    num_heads: [4, 4]\n",
      "    window_size: 8\n",
      "    use_checkpoint: True\n",
      "  ]\n",
      "  network_map:[\n",
      "    type: FlowSimCorrespondenceGenerationArch\n",
      "    patch_size: 3\n",
      "    stride: 1\n",
      "    vgg_layer_list: ['relu1_1', 'relu2_1', 'relu3_1']\n",
      "    vgg_type: vgg19\n",
      "  ]\n",
      "  network_extractor:[\n",
      "    type: ContrasExtractorSep\n",
      "  ]\n",
      "  path:[\n",
      "    pretrain_model_g: experiments/pretrained_model/restoration_mse.pth\n",
      "    pretrain_model_feature_extractor: experiments/feature_extraction.pth\n",
      "    strict_load: True\n",
      "    root: experiments/test/\n",
      "    results_root: experiments/test/results\\test_restoration_mse\n",
      "    log: experiments/test/results\\test_restoration_mse\n",
      "    visualization: experiments/test/results\\test_restoration_mse\\visualization\n",
      "  ]\n",
      "  is_train: False\n",
      "  dist: False\n",
      "\n",
      "2024-11-17 13:05:33,090.090 - INFO: Dataset RefCUFEDDataset - WR-SR is created.\n",
      "2024-11-17 13:05:33,090.090 - INFO: Number of test images in WR-SR: 2\n",
      "C:\\Users\\dheer\\.conda\\envs\\mmcv\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:3191.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "2024-11-17 13:05:34,940.940 - INFO: Network: DataParallel - SwinUnetv3RestorationNet, with parameters: 18,026,971\n",
      "2024-11-17 13:05:34,940.940 - INFO: SwinUnetv3RestorationNet(\n",
      "  (content_extractor): ContentExtractor(\n",
      "    (conv_first): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (body): Sequential(\n",
      "      (0): ResidualBlockNoBN(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): ResidualBlockNoBN(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): ResidualBlockNoBN(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): ResidualBlockNoBN(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): ResidualBlockNoBN(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): ResidualBlockNoBN(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (6): ResidualBlockNoBN(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (7): ResidualBlockNoBN(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (lrelu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      "  (dyn_agg_restore): DynamicAggregationRestoration(\n",
      "    (unet_head): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (down_large_offset_conv1): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (down_large_offset_conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (down_large_dyn_agg): DCN_sep_pre_multi_offset_flow_similarity(\n",
      "      (conv_offset_mask): Conv2d(64, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (down_head_large): Sequential(\n",
      "      (0): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (down_body_large): SwinBlock(\n",
      "      (patch_embed): PatchEmbed(\n",
      "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (patch_unembed): PatchUnEmbed()\n",
      "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): RSTB(\n",
      "          (residual_group): BasicLayer(\n",
      "            dim=128, input_resolution=(160, 160), depth=4\n",
      "            (blocks): ModuleList(\n",
      "              (0): SwinTransformerBlock(\n",
      "                dim=128, input_resolution=(160, 160), num_heads=4, window_size=8, shift_size=0, mlp_ratio=2.0\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=128, window_size=(8, 8), num_heads=4\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): SwinTransformerBlock(\n",
      "                dim=128, input_resolution=(160, 160), num_heads=4, window_size=8, shift_size=4, mlp_ratio=2.0\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=128, window_size=(8, 8), num_heads=4\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.014)\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (2): SwinTransformerBlock(\n",
      "                dim=128, input_resolution=(160, 160), num_heads=4, window_size=8, shift_size=0, mlp_ratio=2.0\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=128, window_size=(8, 8), num_heads=4\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.029)\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (3): SwinTransformerBlock(\n",
      "                dim=128, input_resolution=(160, 160), num_heads=4, window_size=8, shift_size=4, mlp_ratio=2.0\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=128, window_size=(8, 8), num_heads=4\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.043)\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (patch_embed): PatchEmbed()\n",
      "          (patch_unembed): PatchUnEmbed()\n",
      "        )\n",
      "        (1): RSTB(\n",
      "          (residual_group): BasicLayer(\n",
      "            dim=128, input_resolution=(160, 160), depth=4\n",
      "            (blocks): ModuleList(\n",
      "              (0): SwinTransformerBlock(\n",
      "                dim=128, input_resolution=(160, 160), num_heads=4, window_size=8, shift_size=0, mlp_ratio=2.0\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=128, window_size=(8, 8), num_heads=4\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.057)\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): SwinTransformerBlock(\n",
      "                dim=128, input_resolution=(160, 160), num_heads=4, window_size=8, shift_size=4, mlp_ratio=2.0\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=128, window_size=(8, 8), num_heads=4\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.071)\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (2): SwinTransformerBlock(\n",
      "                dim=128, input_resolution=(160, 160), num_heads=4, window_size=8, shift_size=0, mlp_ratio=2.0\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=128, window_size=(8, 8), num_heads=4\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.086)\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (3): SwinTransformerBlock(\n",
      "                dim=128, input_resolution=(160, 160), num_heads=4, window_size=8, shift_size=4, mlp_ratio=2.0\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=128, window_size=(8, 8), num_heads=4\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.100)\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (patch_embed): PatchEmbed()\n",
      "          (patch_unembed): PatchUnEmbed()\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (down_tail_large): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (down_medium_offset_conv1): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (down_medium_offset_conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (down_medium_dyn_agg): DCN_sep_pre_multi_offset_flow_similarity(\n",
      "      (conv_offset_mask): Conv2d(128, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (down_head_medium): Sequential(\n",
      "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (down_body_medium): SwinBlock(\n",
      "      (patch_embed): PatchEmbed(\n",
      "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (patch_unembed): PatchUnEmbed()\n",
      "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): RSTB(\n",
      "          (residual_group): BasicLayer(\n",
      "            dim=128, input_resolution=(80, 80), depth=4\n",
      "            (blocks): ModuleList(\n",
      "              (0): SwinTransformerBlock(\n",
      "                dim=128, input_resolution=(80, 80), num_heads=4, window_size=8, shift_size=0, mlp_ratio=2.0\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=128, window_size=(8, 8), num_heads=4\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): SwinTransformerBlock(\n",
      "                dim=128, input_resolution=(80, 80), num_heads=4, window_size=8, shift_size=4, mlp_ratio=2.0\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=128, window_size=(8, 8), num_heads=4\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.014)\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (2): SwinTransformerBlock(\n",
      "                dim=128, input_resolution=(80, 80), num_heads=4, window_size=8, shift_size=0, mlp_ratio=2.0\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=128, window_size=(8, 8), num_heads=4\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.029)\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (3): SwinTransformerBlock(\n",
      "                dim=128, input_resolution=(80, 80), num_heads=4, window_size=8, shift_size=4, mlp_ratio=2.0\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=128, window_size=(8, 8), num_heads=4\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.043)\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (patch_embed): PatchEmbed()\n",
      "          (patch_unembed): PatchUnEmbed()\n",
      "        )\n",
      "        (1): RSTB(\n",
      "          (residual_group): BasicLayer(\n",
      "            dim=128, input_resolution=(80, 80), depth=4\n",
      "            (blocks): ModuleList(\n",
      "              (0): SwinTransformerBlock(\n",
      "                dim=128, input_resolution=(80, 80), num_heads=4, window_size=8, shift_size=0, mlp_ratio=2.0\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=128, window_size=(8, 8), num_heads=4\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.057)\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): SwinTransformerBlock(\n",
      "                dim=128, input_resolution=(80, 80), num_heads=4, window_size=8, shift_size=4, mlp_ratio=2.0\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=128, window_size=(8, 8), num_heads=4\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.071)\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (2): SwinTransformerBlock(\n",
      "                dim=128, input_resolution=(80, 80), num_heads=4, window_size=8, shift_size=0, mlp_ratio=2.0\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=128, window_size=(8, 8), num_heads=4\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.086)\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (3): SwinTransformerBlock(\n",
      "                dim=128, input_resolution=(80, 80), num_heads=4, window_size=8, shift_size=4, mlp_ratio=2.0\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=128, window_size=(8, 8), num_heads=4\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.100)\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (patch_embed): PatchEmbed()\n",
      "          (patch_unembed): PatchUnEmbed()\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (down_tail_medium): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (up_small_offset_conv1): Conv2d(640, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (up_small_offset_conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (up_small_dyn_agg): DCN_sep_pre_multi_offset_flow_similarity(\n",
      "      (conv_offset_mask): Conv2d(256, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (up_head_small): Sequential(\n",
      "      (0): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (up_body_small): SwinBlock(\n",
      "      (patch_embed): PatchEmbed(\n",
      "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (patch_unembed): PatchUnEmbed()\n",
      "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): RSTB(\n",
      "          (residual_group): BasicLayer(\n",
      "            dim=128, input_resolution=(40, 40), depth=4\n",
      "            (blocks): ModuleList(\n",
      "              (0): SwinTransformerBlock(\n",
      "                dim=128, input_resolution=(40, 40), num_heads=4, window_size=8, shift_size=0, mlp_ratio=2.0\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=128, window_size=(8, 8), num_heads=4\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): SwinTransformerBlock(\n",
      "                dim=128, input_resolution=(40, 40), num_heads=4, window_size=8, shift_size=4, mlp_ratio=2.0\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=128, window_size=(8, 8), num_heads=4\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.014)\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (2): SwinTransformerBlock(\n",
      "                dim=128, input_resolution=(40, 40), num_heads=4, window_size=8, shift_size=0, mlp_ratio=2.0\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=128, window_size=(8, 8), num_heads=4\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.029)\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (3): SwinTransformerBlock(\n",
      "                dim=128, input_resolution=(40, 40), num_heads=4, window_size=8, shift_size=4, mlp_ratio=2.0\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=128, window_size=(8, 8), num_heads=4\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.043)\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (patch_embed): PatchEmbed()\n",
      "          (patch_unembed): PatchUnEmbed()\n",
      "        )\n",
      "        (1): RSTB(\n",
      "          (residual_group): BasicLayer(\n",
      "            dim=128, input_resolution=(40, 40), depth=4\n",
      "            (blocks): ModuleList(\n",
      "              (0): SwinTransformerBlock(\n",
      "                dim=128, input_resolution=(40, 40), num_heads=4, window_size=8, shift_size=0, mlp_ratio=2.0\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=128, window_size=(8, 8), num_heads=4\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.057)\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): SwinTransformerBlock(\n",
      "                dim=128, input_resolution=(40, 40), num_heads=4, window_size=8, shift_size=4, mlp_ratio=2.0\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=128, window_size=(8, 8), num_heads=4\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.071)\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (2): SwinTransformerBlock(\n",
      "                dim=128, input_resolution=(40, 40), num_heads=4, window_size=8, shift_size=0, mlp_ratio=2.0\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=128, window_size=(8, 8), num_heads=4\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.086)\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (3): SwinTransformerBlock(\n",
      "                dim=128, input_resolution=(40, 40), num_heads=4, window_size=8, shift_size=4, mlp_ratio=2.0\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=128, window_size=(8, 8), num_heads=4\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.100)\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (patch_embed): PatchEmbed()\n",
      "          (patch_unembed): PatchUnEmbed()\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (up_tail_small): Sequential(\n",
      "      (0): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): PixelShuffle(upscale_factor=2)\n",
      "      (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (up_medium_offset_conv1): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (up_medium_offset_conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (up_medium_dyn_agg): DCN_sep_pre_multi_offset_flow_similarity(\n",
      "      (conv_offset_mask): Conv2d(128, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (up_head_medium): Sequential(\n",
      "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (up_body_medium): SwinBlock(\n",
      "      (patch_embed): PatchEmbed(\n",
      "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (patch_unembed): PatchUnEmbed()\n",
      "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): RSTB(\n",
      "          (residual_group): BasicLayer(\n",
      "            dim=128, input_resolution=(80, 80), depth=4\n",
      "            (blocks): ModuleList(\n",
      "              (0): SwinTransformerBlock(\n",
      "                dim=128, input_resolution=(80, 80), num_heads=4, window_size=8, shift_size=0, mlp_ratio=2.0\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=128, window_size=(8, 8), num_heads=4\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): SwinTransformerBlock(\n",
      "                dim=128, input_resolution=(80, 80), num_heads=4, window_size=8, shift_size=4, mlp_ratio=2.0\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=128, window_size=(8, 8), num_heads=4\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.014)\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (2): SwinTransformerBlock(\n",
      "                dim=128, input_resolution=(80, 80), num_heads=4, window_size=8, shift_size=0, mlp_ratio=2.0\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=128, window_size=(8, 8), num_heads=4\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.029)\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (3): SwinTransformerBlock(\n",
      "                dim=128, input_resolution=(80, 80), num_heads=4, window_size=8, shift_size=4, mlp_ratio=2.0\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=128, window_size=(8, 8), num_heads=4\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.043)\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (patch_embed): PatchEmbed()\n",
      "          (patch_unembed): PatchUnEmbed()\n",
      "        )\n",
      "        (1): RSTB(\n",
      "          (residual_group): BasicLayer(\n",
      "            dim=128, input_resolution=(80, 80), depth=4\n",
      "            (blocks): ModuleList(\n",
      "              (0): SwinTransformerBlock(\n",
      "                dim=128, input_resolution=(80, 80), num_heads=4, window_size=8, shift_size=0, mlp_ratio=2.0\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=128, window_size=(8, 8), num_heads=4\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.057)\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): SwinTransformerBlock(\n",
      "                dim=128, input_resolution=(80, 80), num_heads=4, window_size=8, shift_size=4, mlp_ratio=2.0\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=128, window_size=(8, 8), num_heads=4\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.071)\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (2): SwinTransformerBlock(\n",
      "                dim=128, input_resolution=(80, 80), num_heads=4, window_size=8, shift_size=0, mlp_ratio=2.0\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=128, window_size=(8, 8), num_heads=4\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.086)\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (3): SwinTransformerBlock(\n",
      "                dim=128, input_resolution=(80, 80), num_heads=4, window_size=8, shift_size=4, mlp_ratio=2.0\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=128, window_size=(8, 8), num_heads=4\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.100)\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (patch_embed): PatchEmbed()\n",
      "          (patch_unembed): PatchUnEmbed()\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (up_tail_medium): Sequential(\n",
      "      (0): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): PixelShuffle(upscale_factor=2)\n",
      "      (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (up_large_offset_conv1): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (up_large_offset_conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (up_large_dyn_agg): DCN_sep_pre_multi_offset_flow_similarity(\n",
      "      (conv_offset_mask): Conv2d(64, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (up_head_large): Sequential(\n",
      "      (0): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    )\n",
      "    (up_body_large): SwinBlock(\n",
      "      (patch_embed): PatchEmbed(\n",
      "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (patch_unembed): PatchUnEmbed()\n",
      "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): RSTB(\n",
      "          (residual_group): BasicLayer(\n",
      "            dim=128, input_resolution=(160, 160), depth=4\n",
      "            (blocks): ModuleList(\n",
      "              (0): SwinTransformerBlock(\n",
      "                dim=128, input_resolution=(160, 160), num_heads=4, window_size=8, shift_size=0, mlp_ratio=2.0\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=128, window_size=(8, 8), num_heads=4\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): Identity()\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): SwinTransformerBlock(\n",
      "                dim=128, input_resolution=(160, 160), num_heads=4, window_size=8, shift_size=4, mlp_ratio=2.0\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=128, window_size=(8, 8), num_heads=4\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.014)\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (2): SwinTransformerBlock(\n",
      "                dim=128, input_resolution=(160, 160), num_heads=4, window_size=8, shift_size=0, mlp_ratio=2.0\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=128, window_size=(8, 8), num_heads=4\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.029)\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (3): SwinTransformerBlock(\n",
      "                dim=128, input_resolution=(160, 160), num_heads=4, window_size=8, shift_size=4, mlp_ratio=2.0\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=128, window_size=(8, 8), num_heads=4\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.043)\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (patch_embed): PatchEmbed()\n",
      "          (patch_unembed): PatchUnEmbed()\n",
      "        )\n",
      "        (1): RSTB(\n",
      "          (residual_group): BasicLayer(\n",
      "            dim=128, input_resolution=(160, 160), depth=4\n",
      "            (blocks): ModuleList(\n",
      "              (0): SwinTransformerBlock(\n",
      "                dim=128, input_resolution=(160, 160), num_heads=4, window_size=8, shift_size=0, mlp_ratio=2.0\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=128, window_size=(8, 8), num_heads=4\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.057)\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (1): SwinTransformerBlock(\n",
      "                dim=128, input_resolution=(160, 160), num_heads=4, window_size=8, shift_size=4, mlp_ratio=2.0\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=128, window_size=(8, 8), num_heads=4\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.071)\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (2): SwinTransformerBlock(\n",
      "                dim=128, input_resolution=(160, 160), num_heads=4, window_size=8, shift_size=0, mlp_ratio=2.0\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=128, window_size=(8, 8), num_heads=4\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.086)\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (3): SwinTransformerBlock(\n",
      "                dim=128, input_resolution=(160, 160), num_heads=4, window_size=8, shift_size=4, mlp_ratio=2.0\n",
      "                (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (attn): WindowAttention(\n",
      "                  dim=128, window_size=(8, 8), num_heads=4\n",
      "                  (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
      "                  (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                  (softmax): Softmax(dim=-1)\n",
      "                )\n",
      "                (drop_path): DropPath(drop_prob=0.100)\n",
      "                (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "                (mlp): Mlp(\n",
      "                  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
      "                  (act): GELU(approximate='none')\n",
      "                  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "                  (drop): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (patch_embed): PatchEmbed()\n",
      "          (patch_unembed): PatchUnEmbed()\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (up_tail_large): Sequential(\n",
      "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "      (2): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (lrelu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      ")\n",
      "2024-11-17 13:05:34,940.940 - INFO: Loading SwinUnetv3RestorationNet model from experiments/pretrained_model/restoration_mse.pth.\n",
      "C:\\Users\\dheer\\.conda\\envs\\mmcv\\lib\\site-packages\\torchvision\\models\\_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n",
      "C:\\Users\\dheer\\.conda\\envs\\mmcv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\dheer\\.conda\\envs\\mmcv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "2024-11-17 13:05:45,733.733 - INFO: Network: DataParallel - ContrasExtractorSep, with parameters: 1,110,656\n",
      "2024-11-17 13:05:45,736.736 - INFO: ContrasExtractorSep(\n",
      "  (feature_extraction_image1): ContrasExtractorLayer(\n",
      "    (model): Sequential(\n",
      "      (conv1_1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (relu1_1): ReLU(inplace=True)\n",
      "      (conv1_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (relu1_2): ReLU(inplace=True)\n",
      "      (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (conv2_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (relu2_1): ReLU(inplace=True)\n",
      "      (conv2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (relu2_2): ReLU(inplace=True)\n",
      "      (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (conv3_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (feature_extraction_image2): ContrasExtractorLayer(\n",
      "    (model): Sequential(\n",
      "      (conv1_1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (relu1_1): ReLU(inplace=True)\n",
      "      (conv1_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (relu1_2): ReLU(inplace=True)\n",
      "      (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (conv2_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (relu2_1): ReLU(inplace=True)\n",
      "      (conv2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (relu2_2): ReLU(inplace=True)\n",
      "      (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (conv3_1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "2024-11-17 13:05:45,736.736 - INFO: Loading ContrasExtractorSep model from experiments/feature_extraction.pth.\n",
      "2024-11-17 13:05:45,783.783 - INFO: Loading SwinUnetv3RestorationNet model from experiments/pretrained_model/restoration_mse.pth.\n",
      "2024-11-17 13:05:46,353.353 - INFO: Model [RefRestorationModel] is created.\n",
      "2024-11-17 13:05:46,353.353 - INFO: Testing WR-SR...\n",
      "C:\\Users\\dheer\\.conda\\envs\\mmcv\\lib\\site-packages\\torch\\utils\\checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "2024-11-17 13:09:11,034.034 - INFO: # img 001_ref # PSNR: 2.5187e+01 # PSNR_Y: 2.6554e+01 # SSIM_Y: 8.3173e-01.\n",
      "2024-11-17 13:16:26,312.312 - INFO: # img 002_ref # PSNR: 2.6157e+01 # PSNR_Y: 2.7512e+01 # SSIM_Y: 8.3108e-01.\n",
      "C:\\Users\\dheer\\.conda\\envs\\mmcv\\lib\\site-packages\\mmcv\\__init__.py:21: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  'On January 1, 2023, MMCV will release v2.0.0, in which it will remove '\n",
      "2024-11-17 13:16:27,102.102 - INFO: # Validation WR-SR # PSNR: 2.5672e+01 # PSNR_Y: 2.7033e+01 # SSIM_Y: 8.3140e-01.\n"
     ]
    }
   ],
   "source": [
    "!python datsr/test.py -opt \"options/test/test_restoration_mse.yml\" --launcher \"none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05d1155-b423-4c76-a045-98ed3987a498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from datsr.models.archs.datsr_arch import DATSR\n",
    "from datsr.test.\n",
    "\n",
    "# Grayscale transformation function\n",
    "def grayscale_transform(image):\n",
    "    image = image.squeeze(0).cpu().numpy().transpose(1, 2, 0)\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    return torch.tensor(gray_image).unsqueeze(0).cuda()\n",
    "\n",
    "# Whitening and Coloring Transform (WCT) function\n",
    "def wct(content_features, style_features):\n",
    "    c_mean = content_features.mean(dim=[2, 3], keepdim=True)\n",
    "    s_mean = style_features.mean(dim=[2, 3], keepdim=True)\n",
    "\n",
    "    content_centered = content_features - c_mean\n",
    "    style_centered = style_features - s_mean\n",
    "\n",
    "    c_cov = content_centered @ content_centered.permute(0, 1, 3, 2)\n",
    "    s_cov = style_centered @ style_centered.permute(0, 1, 3, 2)\n",
    "\n",
    "    c_eigval, c_eigvec = torch.symeig(c_cov, eigenvectors=True)\n",
    "    s_eigval, s_eigvec = torch.symeig(s_cov, eigenvectors=True)\n",
    "\n",
    "    whitening = c_eigvec @ torch.diag_embed(torch.sqrt(1 / (c_eigval + 1e-5))) @ c_eigvec.permute(0, 1, 3, 2)\n",
    "    coloring = s_eigvec @ torch.diag_embed(torch.sqrt(s_eigval + 1e-5)) @ s_eigvec.permute(0, 1, 3, 2)\n",
    "\n",
    "    transformed_features = coloring @ (whitening @ content_centered) + s_mean\n",
    "    return transformed_features\n",
    "\n",
    "# Phase Replacement (PR) function\n",
    "def phase_replacement(content_feature, stylized_feature):\n",
    "    content_fft = torch.fft.fft2(content_feature, dim=(-2, -1))\n",
    "    stylized_fft = torch.fft.fft2(stylized_feature, dim=(-2, -1))\n",
    "\n",
    "    amplitude_content = torch.abs(content_fft)\n",
    "    phase_content = torch.angle(content_fft)\n",
    "\n",
    "    amplitude_stylized = torch.abs(stylized_fft)\n",
    "\n",
    "    result = amplitude_stylized * torch.exp(1j * phase_content)\n",
    "    return torch.fft.ifft2(result, dim=(-2, -1)).real\n",
    "\n",
    "# Domain Matching Module\n",
    "class DomainMatchingSR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DomainMatchingSR, self).__init__()\n",
    "        self.encoder = models.vgg19(pretrained=True).features[:21]  # Use VGG19 for feature extraction\n",
    "\n",
    "    def forward(self, lr_image, ref_image):\n",
    "        # Step 1: Grayscale Transformation\n",
    "        lr_gray = grayscale_transform(lr_image)\n",
    "        ref_gray = grayscale_transform(ref_image)\n",
    "\n",
    "        # Step 2: Feature Extraction (with VGG19)\n",
    "        lr_features = self.encoder(lr_gray)\n",
    "        ref_features = self.encoder(ref_gray)\n",
    "\n",
    "        # Step 3: Whitening and Coloring Transform\n",
    "        transformed_features = wct(lr_features, ref_features)\n",
    "\n",
    "        # Step 4: Phase Replacement\n",
    "        output = phase_replacement(lr_features, transformed_features)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Integrated DATSR with Domain Matching\n",
    "class IntegratedDATSR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IntegratedDATSR, self).__init__()\n",
    "        self.datsr = DATSR()  # Load the existing DATSR model\n",
    "        self.domain_matching = DomainMatchingSR()\n",
    "\n",
    "    def forward(self, lr_image, ref_image):\n",
    "        # Apply Domain Matching Module\n",
    "        domain_matched_image = self.domain_matching(lr_image, ref_image)\n",
    "\n",
    "        # Pass through DATSR\n",
    "        output_sr = self.datsr(domain_matched_image)\n",
    "\n",
    "        return output_sr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
